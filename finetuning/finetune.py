import mlflow
import mlflow.pytorch
import mlflow.sentence_transformers
import psycopg2
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader

mlflow.pytorch.autolog()                # Logs PyTorch model, optimizer, loss, metrics
mlflow.transformers.autolog()           # Suppresses irrelevant sub-module logs (for HF version 4.35.2â€“4.51.2) 
                                       # (Experimental feature) :contentReference[oaicite:6]{index=6}

conn = None

def connect_to_database():
    """Establish connection to PostgreSQL database."""
    if conn is not None:
        return conn

    try:
        conn = psycopg2.connect("postgresql://postgres.ifhsgnwyvpjwiwxkqpfc:poridhi@aws-0-us-west-1.pooler.supabase.com:6543/postgres")
        print("Successfully connected to PostgreSQL.")
        return conn
    except psycopg2.Error as e:
        print(f"Error connecting to PostgreSQL: {e}")
        return None


cursor = conn.cursor()

# Fetch data
cursor.execute("""
    SELECT query, positive, weight
    FROM public.finetune_data
    WHERE query IS NOT NULL AND positive IS NOT NULL
""")
rows = cursor.fetchall()

# Create InputExamples
train_examples = []
for query, positive, weight in rows:
    print(positive)
    # Fallback if weight is NULL (even though you have default 1.0)
    label = weight if weight is not None else 1.0
    train_examples.append(
        InputExample(texts=[query, positive], label=label)
    )

cursor.close()
conn.close()
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)

# Load base model
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# Define loss
train_loss = losses.CosineSimilarityLoss(model)

# Start MLflow run
with mlflow.start_run(run_name="st_finetune"):

    # Log hyperparameters
    mlflow.log_param("model_name", "all-MiniLM-L6-v2")
    mlflow.log_param("batch_size", 16)
    mlflow.log_param("loss", "CosineSimilarityLoss")
    mlflow.log_param("epochs", 3)

    # Train model
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=3,
        warmup_steps=100
    )

    # Log the fine-tuned model artifact
    mlflow.sentence_transformers.log_model(
        model,
        artifact_path="fine_tuned_model",
        task="llm/v1/embeddings"  # enables an OpenAI-compatible embeddings interface
    )


# create table public.finetune_data (
#   id bigint generated by default as identity not null,
#   created_at timestamp with time zone not null default now(),
#   query text null,
#   positive text null,
#   weight double precision null default '1'::double precision,
#   constraint finetune_data_pkey primary key (id)
# ) TABLESPACE pg_default;