{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1afcf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "684369eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-mistralai in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (0.2.10)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from langchain-mistralai) (0.3.55)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15.1 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from langchain-mistralai) (0.21.1)\n",
      "Requirement already satisfied: httpx<1,>=0.25.2 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from langchain-mistralai) (0.28.1)\n",
      "Requirement already satisfied: httpx-sse<1,>=0.3.1 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from langchain-mistralai) (0.4.0)\n",
      "Requirement already satisfied: pydantic<3,>=2 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from langchain-mistralai) (2.11.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (1.0.8)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain-mistralai) (0.14.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-mistralai) (0.3.33)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-mistralai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-mistralai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-mistralai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-mistralai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-mistralai) (4.13.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from pydantic<3,>=2->langchain-mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from pydantic<3,>=2->langchain-mistralai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from pydantic<3,>=2->langchain-mistralai) (0.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from tokenizers<1,>=0.15.1->langchain-mistralai) (0.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (2025.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (4.67.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain-mistralai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-mistralai) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-mistralai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-mistralai) (0.23.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from anyio->httpx<1,>=0.25.2->langchain-mistralai) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (2.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\desktop\\ai engg\\codes\\poridhihackathon\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65f41191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4aef4ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0lzefmPJKlVKaL8hWROiGjWV22P1itTN'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f9175bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mistralai import Mistral\n",
    "\n",
    "# model = \"ministral-8b-latest\"\n",
    "\n",
    "# client = Mistral(api_key=api_key)\n",
    "\n",
    "# chat_response = client.chat.complete(\n",
    "#     model= model,\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"What is the best French cheese?\",\n",
    "#         },\n",
    "#     ]\n",
    "# )\n",
    "# print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82ecf13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bfc307",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79eed4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me encanta programar.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-small-latest\",\n",
    "    temperature=0.3,\n",
    "    max_retries=5\n",
    ")\n",
    "\n",
    "# Prompts\n",
    "sys_prompt = \"You are a helpful assistant that translates English to Spanish\"\n",
    "hum_prompt = \"Translate the user sentence without giving more details, just the translation: I love programming.\"\n",
    "\n",
    "# Invoke LLM\n",
    "response = llm.invoke([\n",
    "    SystemMessage(content=sys_prompt),\n",
    "    HumanMessage(content=hum_prompt)]\n",
    ").content.strip()\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739158c5",
   "metadata": {},
   "source": [
    "## structuring agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61776092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Any\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define our state\n",
    "class AgentState(BaseModel):\n",
    "    messages: List[Any] = Field(default_factory=list)\n",
    "    formatted_response: str = \"\"\n",
    "\n",
    "# Initialize MistralAI LLM\n",
    "def initialize_llm(api_key=None):\n",
    "    if api_key:\n",
    "        os.environ[\"MISTRAL_API_KEY\"] = api_key\n",
    "    \n",
    "    # Make sure MISTRAL_API_KEY is set in environment variables if not passed as parameter\n",
    "    return ChatMistralAI(\n",
    "        model=\"mistral-small-latest\",  # You can change the model as needed\n",
    "        temperature=0.1  # Low temperature for more deterministic responses\n",
    "    )\n",
    "\n",
    "# Define the system prompt that contains formatting instructions\n",
    "def create_system_prompt(formatting_instructions):\n",
    "    return SystemMessage(content=f\"\"\"\n",
    "You are a query rewriter agent that takes raw user and rewrites the query in a concise manner so that the information in the following fields are highlighted:\n",
    "\n",
    "{formatting_instructions}\n",
    "\n",
    "Just give the rewritten query. Do not include any explanations or additional text in your response.\n",
    "Just provide the rewritten result.\n",
    "\"\"\")\n",
    "\n",
    "# Node to format the query\n",
    "def format_query(state: AgentState) -> AgentState:\n",
    "    llm = initialize_llm()\n",
    "    \n",
    "    # Get the most recent user message\n",
    "    user_message = state.messages[-1]\n",
    "    \n",
    "    # Include system message and user message for the LLM call\n",
    "    system_message = state.messages[0]\n",
    "    messages = [system_message, user_message]\n",
    "    \n",
    "    # Get formatted response from LLM\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Update state with formatted response\n",
    "    state.formatted_response = response.content\n",
    "    return state\n",
    "\n",
    "# Node to prepare final response\n",
    "def prepare_response(state: AgentState) -> AgentState:\n",
    "    # Simply pass through the formatted response\n",
    "    return state\n",
    "\n",
    "# Create the graph\n",
    "def create_formatting_agent(formatting_instructions):\n",
    "    # Create system prompt\n",
    "    system_prompt = create_system_prompt(formatting_instructions)\n",
    "    \n",
    "    # Define the graph\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"format_query\", format_query)\n",
    "    workflow.add_node(\"prepare_response\", prepare_response)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(\"format_query\", \"prepare_response\")\n",
    "    workflow.add_edge(\"prepare_response\", END)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"format_query\")\n",
    "    \n",
    "    # Compile the graph\n",
    "    agent = workflow.compile()\n",
    "    \n",
    "    # Return a function that initializes the state and runs the agent\n",
    "    def run_agent(query):\n",
    "        messages = [system_prompt, HumanMessage(content=query)]\n",
    "        result = agent.invoke({\"messages\": messages})\n",
    "        return result\n",
    "    \n",
    "    return run_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e991ebbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggest some affordable cars.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your formatting instructions\n",
    "formatting_instructions = \"\"\"\n",
    "Fields : Car Company, Car model, Year of production, Mileage, Price, Color, Type of fuel, Engine Transmission, Engine Capacity\n",
    "\"\"\"\n",
    "\n",
    "# Create the agent\n",
    "formatting_agent = create_formatting_agent(formatting_instructions)\n",
    "\n",
    "# Test with a sample query\n",
    "query = \"I'm a student. I dont earn that much. Suggest me some cars that I can buy\"\n",
    "result = formatting_agent(query)\n",
    "print(result['formatted_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227fa70f",
   "metadata": {},
   "source": [
    "## grading agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "19dd2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Any, Optional\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define our state\n",
    "class GradingAgentState(BaseModel):\n",
    "    messages: List[Any] = Field(default_factory=list)\n",
    "    query: str = \"\"\n",
    "    results: List[str] = Field(default_factory=list)\n",
    "    graded_results: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "\n",
    "# Initialize MistralAI LLM\n",
    "def initialize_llm(api_key=None):\n",
    "    if api_key:\n",
    "        os.environ[\"MISTRAL_API_KEY\"] = api_key\n",
    "    \n",
    "    # Make sure MISTRAL_API_KEY is set in environment variables if not passed as parameter\n",
    "    return ChatMistralAI(\n",
    "        model=\"mistral-large-latest\",  # You can change the model as needed\n",
    "        temperature=0.1  # Low temperature for more deterministic grading\n",
    "    )\n",
    "\n",
    "# Define the system prompt for the grading agent\n",
    "def create_system_prompt():\n",
    "    return SystemMessage(content=\"\"\"\n",
    "You are a grading agent that evaluates the accuracy of search results based on a user query.\n",
    "\n",
    "For each result, you will assign one of the following grades:\n",
    "- 2: Accurate - The result directly and correctly answers the query\n",
    "- 1: Moderately Accurate - The result is partially relevant or contains some useful information related to the query\n",
    "- 0: Inaccurate - The result is irrelevant, contains incorrect information, or doesn't address the query\n",
    "\n",
    "Your response should be formatted as follows:\n",
    "```\n",
    "[\n",
    "  {\"result_index\": 0, \"grade\": <2|1|0>},\n",
    "  {\"result_index\": 1, \"grade\": <2|1|0>},\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "Do not include any additional text, commentary, or explanations outside this JSON structure.\n",
    "Focus solely on the accuracy and relevancy of each result in relation to the query.\n",
    "\"\"\")\n",
    "\n",
    "# Node to process the input query and results\n",
    "def process_input(state: GradingAgentState) -> GradingAgentState:\n",
    "    # The query and results are already set in the state from the run_agent function\n",
    "    return state\n",
    "\n",
    "# Node to grade the results\n",
    "def grade_results(state: GradingAgentState) -> GradingAgentState:\n",
    "    llm = initialize_llm()\n",
    "    \n",
    "    # If there are no results, return empty graded_results\n",
    "    if not state.results:\n",
    "        state.graded_results = []\n",
    "        return state\n",
    "    \n",
    "    # Prepare the message for the LLM\n",
    "    system_message = create_system_prompt()\n",
    "    \n",
    "    prompt_content = f\"\"\"\n",
    "Query: {state.query}\n",
    "\n",
    "Results to grade:\n",
    "\"\"\"\n",
    "    \n",
    "    for i, result in enumerate(state.results):\n",
    "        prompt_content += f\"\\nResult {i}:\\n{result}\\n\"\n",
    "    \n",
    "    prompt_message = HumanMessage(content=prompt_content)\n",
    "    \n",
    "    # Get grading from LLM\n",
    "    response = llm.invoke([system_message, prompt_message])\n",
    "    \n",
    "    # Parse the response to get the graded results\n",
    "    try:\n",
    "        # Extract JSON from the response\n",
    "        response_content = response.content\n",
    "        \n",
    "        # If the response is wrapped in code blocks, extract the content\n",
    "        if \"```\" in response_content:\n",
    "            response_content = response_content.split(\"```\")[1]\n",
    "            if response_content.startswith(\"json\"):\n",
    "                response_content = response_content[4:].strip()\n",
    "        \n",
    "        import json\n",
    "        graded_results = json.loads(response_content)\n",
    "        state.graded_results = graded_results\n",
    "    except Exception as e:\n",
    "        # If parsing fails, create a fallback response\n",
    "        state.graded_results = [\n",
    "            {\"result_index\": i, \"grade\": 0, \"explanation\": \"Failed to parse grading result\"}\n",
    "            for i in range(len(state.results))\n",
    "        ]\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Node to format the final response\n",
    "def format_response(state: GradingAgentState) -> GradingAgentState:\n",
    "    # The state already contains the graded results, so we can return it as is\n",
    "    return state\n",
    "\n",
    "# Create the grading agent\n",
    "def create_grading_agent():\n",
    "    # Define the graph\n",
    "    workflow = StateGraph(GradingAgentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"process_input\", process_input)\n",
    "    workflow.add_node(\"grade_results\", grade_results)\n",
    "    workflow.add_node(\"format_response\", format_response)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(\"process_input\", \"grade_results\")\n",
    "    workflow.add_edge(\"grade_results\", \"format_response\")\n",
    "    workflow.add_edge(\"format_response\", END)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"process_input\")\n",
    "    \n",
    "    # Compile the graph\n",
    "    agent = workflow.compile()\n",
    "    \n",
    "    # Return a function that takes query and results separately\n",
    "    def run_agent(query: str, results: List[str]):\n",
    "        system_prompt = create_system_prompt()\n",
    "        \n",
    "        # Initialize messages with system prompt\n",
    "        messages = [system_prompt]\n",
    "        print(f\"\")\n",
    "        \n",
    "        # Create initial state with query and results\n",
    "        initial_state = GradingAgentState(\n",
    "            messages=messages,\n",
    "            query=query,\n",
    "            results=results\n",
    "        )\n",
    "        \n",
    "        # Invoke the agent with the initial state\n",
    "        result = agent.invoke(initial_state)\n",
    "        grades_arr = result['graded_results']\n",
    "        graded_query_results = []\n",
    "        for i in range(len(grades_arr)):\n",
    "            obj = {\n",
    "                \"query\" : query,\n",
    "                \"result\" : results[int(grades_arr[i]['result_index'])],\n",
    "                \"grade\" : grades_arr[i]['grade']\n",
    "            }\n",
    "            graded_query_results.append(obj)\n",
    "        \n",
    "        return graded_query_results\n",
    "    \n",
    "    return run_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bf17655f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grading Results:\n",
      "{'query': 'What is the capital of France?', 'result': 'Paris is the capital and most populous city of France.', 'grade': 2} \n",
      "\n",
      "{'query': 'What is the capital of France?', 'result': 'The capital of Italy is Rome, which is known as the Eternal City.', 'grade': 0} \n",
      "\n",
      "{'query': 'What is the capital of France?', 'result': \"France's capital city is Paris, known for the Eiffel Tower and Louvre Museum.\", 'grade': 2} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grading_agent = create_grading_agent()\n",
    "    \n",
    "# Example query and results\n",
    "query = \"What is the capital of France?\"\n",
    "results = [\n",
    "        \"Paris is the capital and most populous city of France.\",\n",
    "        \"The capital of Italy is Rome, which is known as the Eternal City.\",\n",
    "        \"France's capital city is Paris, known for the Eiffel Tower and Louvre Museum.\"\n",
    "    ]\n",
    "\n",
    "    \n",
    "# Run the agent\n",
    "graded_results = grading_agent(query, results)\n",
    "\n",
    "# Display the results\n",
    "print(\"Grading Results:\")\n",
    "for result in graded_results:\n",
    "    # grade_text = {2: \"Accurate\", 1: \"Moderately Accurate\", 0: \"Inaccurate\"}[result[\"grade\"]]\n",
    "    print(result, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poridhiHackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
